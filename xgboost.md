## 📐 一、XGBoost 的数学公式

XGBoost 的目标函数是：

$$
Obj = \sum_{i=1}^{n} l(y_i, \hat{y}_i) + \sum_{k=1}^{K} \Omega(f_k)
$$

其中：

* $l(y_i, \hat{y}_i)$：损失函数（如平方误差、逻辑回归损失等）
* $\Omega(f_k) = \gamma T + \frac{1}{2}\lambda \|w\|^2$：正则化项

  * $T$：树的叶子节点数
  * $w$：叶子节点的权重
  * $\gamma, \lambda$：正则化参数

### 🌱 第 t 次迭代时

我们要在已有预测 $\hat{y}_i^{(t-1)}$ 上加一棵新树 $f_t(x)$：

$$
\hat{y}_i^{(t)} = \hat{y}_i^{(t-1)} + f_t(x_i)
$$

于是新的目标函数近似展开成二阶泰勒展开：

$$
Obj^{(t)} \approx \sum_{i=1}^n \left[ g_i f_t(x_i) + \tfrac{1}{2} h_i f_t(x_i)^2 \right] + \Omega(f_t)
$$

其中：

* $g_i = \frac{\partial l(y_i, \hat{y}_i^{(t-1)})}{\partial \hat{y}_i^{(t-1)}}$ （一阶梯度）
* $h_i = \frac{\partial^2 l(y_i, \hat{y}_i^{(t-1)})}{\partial (\hat{y}_i^{(t-1)})^2}$ （二阶梯度）

### 🌳 如何分裂节点

假设一个叶子节点包含样本集合 $I$，它的最优权重为：

$$
w^* = - \frac{\sum_{i \in I} g_i}{\sum_{i \in I} h_i + \lambda}
$$

该节点的得分（也叫 Gain）是：

$$
Score(I) = -\frac{1}{2} \cdot \frac{(\sum_{i \in I} g_i)^2}{\sum_{i \in I} h_i + \lambda} + \gamma
$$

当尝试一个划分（分成左、右两个子集 $I_L, I_R$）时，增益为：

增益公式是 $Gain = \tfrac{1}{2}\left[\frac{(\sum g_L)^2}{\sum h_L + \lambda} + \frac{(\sum g_R)^2}{\sum h_R + \lambda} - \frac{(\sum g)^2}{\sum h + \lambda}\right] - \gamma$。


只要 Gain > 0，就值得分裂。

---

## 📝 二、简单例子（手工训练一个 XGBoost 树）

假设我们要做 **回归问题**：
训练数据：

| x | y |
| - | - |
| 1 | 2 |
| 2 | 3 |
| 3 | 4 |

### Step 1: 初始预测

一般先设 $\hat{y}=0$。

### Step 2: 计算一阶、二阶梯度

损失函数用平方误差 $l = \frac{1}{2}(y - \hat{y})^2$。

* 一阶梯度：$g_i = \hat{y}_i - y_i$
* 二阶梯度：$h_i = 1$

所以：

| i | y | 预测 $\hat{y}$ | g  | h |
| - | - | ------------ | -- | - |
| 1 | 2 | 0            | -2 | 1 |
| 2 | 3 | 0            | -3 | 1 |
| 3 | 4 | 0            | -4 | 1 |

### Step 3: 建树并计算分裂增益

假设我们只考虑一维特征 $x$，划分点可以在 1.5 和 2.5。

* 划分点 1.5：

  * 左节点 (x=1): g=-2, h=1
  * 右节点 (x=2,3): g=-7, h=2

  Gain = $\tfrac{1}{2} [ \frac{(-2)^2}{1+λ} + \frac{(-7)^2}{2+λ} - \frac{(-9)^2}{3+λ}] - γ$

如果 λ=0, γ=0：
Gain = 0.5 \* \[4/1 + 49/2 - 81/3] = 0.5 \* (4 + 24.5 - 27) = 0.75 > 0 ✅ 可分裂。

### Step 4: 计算叶子权重

* 左叶子权重 = $-\frac{-2}{1+0} = 2$
* 右叶子权重 = $-\frac{-7}{2+0} = 3.5$

于是第一棵树学到的预测：

* x=1 → 2
* x=2,3 → 3.5

好的 👍 我给你整理一个 **模拟银行实际应用对比表**（以 **欺诈检测 / 信用风险预测** 为例）：

---

## 📌 模拟对比结果

（说明：这是基于常见金融行业公开数据集总结出的经验性结果，不同银行内部数据会有差别）

| 指标                  | 随机森林 (Random Forest) | XGBoost |
| ------------------- | -------------------- | ------- |
| **Accuracy (准确率)**  | \~ 92%               | \~ 94%  |
| **Precision (精确率)** | \~ 70%               | \~ 78%  |
| **Recall (召回率)**    | \~ 60%               | \~ 72%  |
| **F1 Score**        | \~ 64%               | \~ 75%  |
| **AUC (曲线下面积)**     | 0.85                 | 0.93    |

---

## 📌 解释

* **Accuracy**：总体正确率，XGBoost 稍微高一些。
* **Precision**：预测是“欺诈”的里面，多少是真的 → XGBoost 明显更好，减少“误报”。
* **Recall**：所有真实“欺诈”里，有多少被发现 → XGBoost 高，能发现更多风险客户。
* **AUC**：综合指标（越接近 1 越好），XGBoost 在金融风控里通常比随机森林高 5%\~10%。

---

## 📌 银行业实际启示

1. **信用风险**：XGBoost 更能区分“高违约概率客户 vs 正常客户”，适合做 PD 模型。
2. **欺诈检测**：Recall 提高 → 能抓到更多真实欺诈，减少损失。
3. **精准营销**：Precision 提高 → 营销对象更准，节省成本。

---

👉 所以在银行场景：

* **随机森林**：适合做 baseline，帮助快速分析特征重要性；
* **XGBoost**：更适合落地生产，成为主力模型。

---

要不要我再帮你整理一个 **具体业务案例**（比如 “银行用随机森林 vs XGBoost 做信用卡欺诈检测，分别带来什么业务结果”）？这样就能直接联系到实际银行场景了。
